from weakref import ref
import requests
from urllib.parse import quote, unquote
import logging
import time
import json
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿— - æ·»åŠ æ–‡ä»¶è¾“å‡º
logging.basicConfig(
    level=logging.INFO,  # è®¾ç½®ä¸ºINFOçº§åˆ«ï¼Œä½†åœ¨ä»£ç ä¸­å…³é”®ä½ç½®ä½¿ç”¨INFOæ—¥å¿—
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("baidu_spider.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BaiduSpider:
    def __init__(self):
        # åˆå§‹åŒ–åŸºæœ¬é…ç½®
        self.base_url = 'https://www.baidu.com/s'
        # ä½¿ç”¨ç”¨æˆ·æä¾›çš„è¯·æ±‚å¤´ - ç§»é™¤accept-encodingé¿å…ç¼–ç é—®é¢˜
        self.headers = {
            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'accept-language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7,en-GB;q=0.6',
            'cache-control': 'max-age=0',
            'connection': 'keep-alive',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0'
        }
        # åˆ›å»ºä¼šè¯å¯¹è±¡
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def search(self, keyword, page=1):
        """
        æ‰§è¡Œç™¾åº¦æœç´¢ï¼Œæ”¯æŒåŠ¨æ€å‚æ•°
        
        Args:
            keyword (str): æœç´¢å…³é”®è¯ï¼ˆåŠ¨æ€å‚æ•°ï¼‰
            page (int): é¡µç 
            
        Returns:
            dict: åŒ…å«å“åº”çŠ¶æ€ã€URLå’Œå“åº”å†…å®¹çš„å­—å…¸
        """
        # å‚æ•°éªŒè¯
        if not keyword or not isinstance(keyword, str):
            logging.error('æœç´¢å…³é”®è¯å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²')
            return {'status': 'error', 'message': 'æœç´¢å…³é”®è¯å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²'}
            
        if page < 1 or not isinstance(page, int):
            logging.error('é¡µç å¿…é¡»æ˜¯å¤§äºç­‰äº1çš„æ•´æ•°')
            return {'status': 'error', 'message': 'é¡µç å¿…é¡»æ˜¯å¤§äºç­‰äº1çš„æ•´æ•°'}
            
        # å¯¹å…³é”®è¯è¿›è¡ŒURLç¼–ç ï¼ˆåŠ¨æ€å‚æ•°å¤„ç†ï¼‰
        encoded_keyword = quote(keyword)
        logging.info(f'åŸå§‹å…³é”®è¯: {keyword}, ç¼–ç å: {encoded_keyword}')
        
        # è®¡ç®—èµ·å§‹ä½ç½®
        start = (page - 1) * 10
        
        # ç›´æ¥å°†åŸå§‹å…³é”®è¯æ¥åœ¨URLå‚æ•°ä½ç½®
        full_url = f"{self.base_url}?wd={encoded_keyword}&pn={start}"
        logging.info(f'å‡†å¤‡è®¿é—®URL: {full_url}')
        
        try:
            # å‘é€è¯·æ±‚ï¼Œç›´æ¥ä½¿ç”¨æ„é€ å¥½çš„URL
            response = self.session.get(
                url=full_url,
                timeout=15
            )
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            response.raise_for_status()
            
            # æ™ºèƒ½å¤„ç†ç¼–ç é—®é¢˜
            # è®©requestsè‡ªåŠ¨æ£€æµ‹ç¼–ç 
            response.encoding = response.apparent_encoding
            # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨UTF-8
            if response.encoding is None or response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            logging.info(f'è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}')
            logging.info(f'å®é™…è®¿é—®çš„URL: {response.url}')
            
            # æ„å»ºå‚æ•°ä¿¡æ¯ç”¨äºè¿”å›
            params_info = {
                'wd': encoded_keyword,  # wdæ˜¯ç™¾åº¦æœç´¢çš„å…³é”®è¯å‚æ•°
                'pn': start            # pnæ˜¯é¡µç åç§»é‡
            }
            
            return {
                'status': 'success',
                'status_code': response.status_code,
                'url': response.url,
                'content': response.text,
                'params': params_info
            }
            
        except Exception as e:
            logging.error(f'è¯·æ±‚å¤±è´¥: {str(e)}')
            return {
                'status': 'error',
                'message': str(e),
                'url': full_url
            }
    
    def extract_search_results(self, html_content):
        """
        ä½¿ç”¨BeautifulSoupä»HTMLä¸­æå–æœç´¢ç»“æœä¿¡æ¯
        
        Args:
            html_content (str): HTMLå“åº”å†…å®¹
            
        Returns:
            list: åŒ…å«æå–ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å«æ ‡é¢˜ã€æ¦‚è¦ã€URLå’Œå°é¢URL
        """
        try:
            # è§£æHTMLå†…å®¹
            soup = BeautifulSoup(html_content, 'html.parser')
            results = []
            
            # æŸ¥æ‰¾æœç´¢ç»“æœé¡¹ï¼ˆç™¾åº¦æœç´¢ç»“æœçš„ä¸»è¦å®¹å™¨ç±»åå¯èƒ½ä¼šå˜åŒ–ï¼‰
            # å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            search_items = soup.select('.result.c-container')
            if not search_items:
                search_items = soup.select('.result-op')
            if not search_items:
                search_items = soup.select('[class*="result"]')
            
            logging.info(f'æ‰¾åˆ° {len(search_items)} ä¸ªæœç´¢ç»“æœé¡¹')
            
            for item in search_items:
                try:
                    # æå–æ ‡é¢˜å’ŒURL
                    title_elem = item.select_one('h3 a') or item.select_one('a')
                    if not title_elem:
                        continue
                        
                    title = title_elem.get_text(strip=True) or ''
                    url = title_elem.get('href') or ''
                    
                    # æå–æ¦‚è¦å†…å®¹
                    summary = ''
                    # å°è¯•å¤šç§å¯èƒ½çš„æ¦‚è¦å…ƒç´ 
                    summary_elem = None
                    
                    # å°è¯•å¤šç§å¯èƒ½çš„æ¦‚è¦é€‰æ‹©å™¨
                    possible_selectors = [
                        '.c-abstract',  # ä¸»è¦çš„æ¦‚è¦ç±»
                        '.c-abstract-size',  # å¯èƒ½çš„å˜ä½“
                        '.content-right',  # å†…å®¹å³ä¾§åŒºåŸŸ
                        '.c-span-text',  # å¯èƒ½åŒ…å«æ–‡æœ¬çš„span
                        'p',  # ç›´æ¥æŸ¥æ‰¾æ®µè½æ ‡ç­¾
                        'div[data-content]',  # å¸¦æ•°æ®å†…å®¹å±æ€§çš„div
                        '.result-op div',  # æ“ä½œç»“æœå†…çš„div
                        '.result-molecule div',  # åˆ†å­ç»“æœå†…çš„div
                        'div[class*="content"]',  # åŒ…å«contentçš„ç±»
                        'div[class*="text"]'  # åŒ…å«textçš„ç±»
                    ]
                    
                    # å°è¯•æ¯ä¸ªé€‰æ‹©å™¨ï¼Œç›´åˆ°æ‰¾åˆ°å†…å®¹
                    for selector in possible_selectors:
                        candidates = item.select(selector)
                        for candidate in candidates:
                            text = candidate.get_text(strip=True)
                            # ç¡®ä¿å†…å®¹ä¸ä¸ºç©ºä¸”é•¿åº¦åˆç†ï¼ˆå¤§äº10ä¸ªå­—ç¬¦ï¼‰
                            if text and len(text) > 10:
                                summary_elem = candidate
                                summary = text
                                break
                        if summary_elem:
                            break
                    
                    logging.info(f'æå–æ¦‚è¦: {summary[:50]}...' if summary else 'æœªæå–åˆ°æ¦‚è¦')
                    
                    # æå–å°é¢URLï¼ˆå¦‚æœæœ‰å›¾ç‰‡ï¼‰
                    cover_url = ''
                    img_elem = item.select_one('img')
                    if img_elem:
                        img_src = img_elem.get('src') or img_elem.get('data-src') or ''
                        # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                        if img_src and not img_src.startswith(('http://', 'https://')):
                            if img_src.startswith('//'):
                                cover_url = f'https:{img_src}'
                            else:
                                cover_url = f'https://www.baidu.com{img_src}'
                        else:
                            cover_url = img_src
                    
                    # åªæ·»åŠ æœ‰æ•ˆç»“æœ
                    if title and url:
                        results.append({
                            'title': title,
                            'summary': summary,
                            'url': url,
                            'cover_url': cover_url
                        })
                        logging.debug(f'æå–åˆ°ç»“æœ: æ ‡é¢˜={title[:30]}...')
                    
                except Exception as e:
                    logging.error(f'å¤„ç†å•ä¸ªæœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}')
                    continue
            
            # å»é‡å¤„ç†ï¼Œé¿å…é‡å¤ç»“æœ
            unique_results = []
            seen_urls = set()
            for result in results:
                if result['url'] not in seen_urls:
                    seen_urls.add(result['url'])
                    unique_results.append(result)
            
            logging.info(f'æˆåŠŸæå–å¹¶å»é‡ {len(unique_results)} æ¡æœç´¢ç»“æœ')
            return unique_results
            
        except Exception as e:
            logging.error(f'è§£æHTMLå¹¶æå–æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}')
            return []
    
    def save_extracted_results(self, extracted_results, keyword):
        """
        å°†æå–çš„æœç´¢ç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶
        
        Args:
            extracted_results (list): æå–çš„æœç´¢ç»“æœåˆ—è¡¨
            keyword (str): æœç´¢å…³é”®è¯
            
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ„å»ºä¿å­˜æ•°æ®çš„ç»“æ„
            save_data = {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'keyword': keyword,
                'total_results': len(extracted_results),
                'results': extracted_results
            }
            
            # ä¿å­˜åˆ°JSONæ–‡ä»¶
            filename = rf'proceeded_pages\search_scan_{keyword}.json'
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(save_data, f, ensure_ascii=False, indent=2)
            
            logging.info(f'æå–çš„æœç´¢ç»“æœå·²ä¿å­˜åˆ° {filename}')
            print(f"\næå–çš„æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            print(f"å…±æå–åˆ° {len(extracted_results)} æ¡æœ‰æ•ˆæœç´¢ç»“æœ")
            
            return True
        except Exception as e:
            logging.error(f'ä¿å­˜æå–çš„æœç´¢ç»“æœå¤±è´¥: {str(e)}')
            print(f"\nâŒ ä¿å­˜æå–çš„æœç´¢ç»“æœå¤±è´¥: {str(e)}")
            return False
    
    def save_response_info(self, result, keyword, page):
        """
        ä¿å­˜å“åº”ä¿¡æ¯åˆ°æ–‡ä»¶
        
        Args:
            result (dict): æœç´¢ç»“æœå­—å…¸
            keyword (str): æœç´¢å…³é”®è¯
            page (int): é¡µç 
        """
        try:
            # ä¿å­˜è¯¦ç»†å“åº”ä¿¡æ¯åˆ°JSONæ–‡ä»¶
            info_filename = rf'search_cache\search_info_{keyword}_{page}.json'
            info_data = {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'keyword': keyword,
                'page': page,
                'status': result.get('status'),
                'status_code': result.get('status_code'),
                'url': result.get('url'),
                'params': result.get('params'),
                'message': result.get('message')
            }
            
            with open(info_filename, 'w', encoding='utf-8') as f:
                json.dump(info_data, f, ensure_ascii=False, indent=2)
            
            logging.info(f'å“åº”ä¿¡æ¯å·²ä¿å­˜åˆ° {info_filename}')
            
            # å¦‚æœæˆåŠŸï¼Œä¿å­˜å®Œæ•´çš„HTMLå†…å®¹
            if result.get('status') == 'success':
                html_filename = rf'html_cache\baidu_search_response_{keyword}_{page}.html'
                # ä¿å­˜å®Œæ•´çš„HTMLå†…å®¹
                with open(html_filename, 'w', encoding='utf-8') as f:
                    f.write(result['content'])
                logging.info(f'å®Œæ•´HTMLå“åº”å·²ä¿å­˜åˆ° {html_filename}')
                
            return True
        except Exception as e:
            logging.error(f'ä¿å­˜å“åº”ä¿¡æ¯å¤±è´¥: {str(e)}')
            return False

def run_spider(keyword, page=1):
    """
    è¿è¡Œçˆ¬è™«çš„ä¸»å‡½æ•° - ä¸“æ³¨äºåŠ¨æ€å‚æ•°å¤„ç†å’Œæ•°æ®æå–
    
    Args:
        keyword (str): æœç´¢å…³é”®è¯ï¼ˆåŠ¨æ€å‚æ•°ï¼‰
        page (int): é¡µç ï¼ˆé»˜è®¤1ï¼Œä¸å†ç”±ç”¨æˆ·è¾“å…¥ï¼‰
        
    Returns:
        dict: æœç´¢ç»“æœä¿¡æ¯
    """
    spider = BaiduSpider()
    
    print(f"\n[åŠ¨æ€å‚æ•°çˆ¬è™«] å¼€å§‹æ‰§è¡Œ")
    print(f"å…³é”®è¯: '{keyword}'")
    print("-" * 50)
    
    try:
        # æ‰§è¡Œæœç´¢ - æ ¸å¿ƒåŠ¨æ€å‚æ•°åŠŸèƒ½
        result = spider.search(keyword, page)
        
        # ä¿å­˜å“åº”ä¿¡æ¯
        spider.save_response_info(result, keyword, page)
        
        # è¾“å‡ºæ‰§è¡Œç»“æœ
        if result['status'] == 'success':
            print(f"\nâœ… è¯·æ±‚æˆåŠŸ!")
            print(f"çŠ¶æ€ç : {result['status_code']}")
            print(f"è®¿é—®URL: {result['url']}")
            print(f"\nåŠ¨æ€å‚æ•°å¤„ç†:")
            print(f"åŸå§‹å…³é”®è¯: {keyword}")
            print(f"ç¼–ç åå…³é”®è¯: {result['params']['wd']}")
            print(f"é¡µç åç§»é‡: {result['params']['pn']}")
            
            # æ˜¾ç¤ºå“åº”å¤§å°ä¿¡æ¯
            content_size = len(result['content'])
            print(f"\nå“åº”å†…å®¹å¤§å°: {content_size} å­—ç¬¦")
            print(f"å“åº”ä¿¡æ¯å·²ä¿å­˜åˆ°: search_info_{keyword}_{page}.json")
            print(f"å®Œæ•´HTMLå†…å®¹å·²ä¿å­˜åˆ°: search_response_{keyword}_{page}.html")
            
            # æå–æœç´¢ç»“æœä¿¡æ¯
            print(f"\nğŸ” æ­£åœ¨æå–æœç´¢ç»“æœä¿¡æ¯...")
            extracted_results = spider.extract_search_results(result['content'])
            
            # ä¿å­˜æå–çš„æœç´¢ç»“æœ
            if extracted_results:
                spider.save_extracted_results(extracted_results, keyword)
                return {"status":"success","result":result,"data":extracted_results}
            else:
                print("\nâš ï¸  æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆæœç´¢ç»“æœ")
                return {"status":"failed","result":result,"data":[]}
                
        else:
            print(f"\nâŒ è¯·æ±‚å¤±è´¥!")
            print(f"é”™è¯¯ä¿¡æ¯: {result['message']}")
            print(f"å°è¯•è®¿é—®çš„URL: {result['url']}")
            return {"status":"failed","result":result,"data":[]}
        
    except Exception as e:
        error_msg = f"çˆ¬è™«æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"\nâŒ {error_msg}")
        logging.error(error_msg)
        return {'status': 'error', 'message': str(e)}

def main(keyword, max_pages=1):
    try:  
        # éªŒè¯å…³é”®è¯
        if not keyword.strip():
            print("é”™è¯¯: å…³é”®è¯ä¸èƒ½ä¸ºç©º")
            return (False, [])
        
        all_results = []
        for page in range(1, max_pages + 1):
            # æ‰§è¡Œçˆ¬è™«
            print("\næ­£åœ¨å‘é€è¯·æ±‚ï¼Œè¯·ç¨å€™...")
            response = run_spider(keyword, page)
            if response["status"] == "success":
                print("\nâœ… æœç´¢æˆåŠŸ!")
                print(f"å…±æå–åˆ° {len(response['data'])} æ¡æœ‰æ•ˆæœç´¢ç»“æœ")
                
                # æ„å»ºè¿”å›æ•°æ®ç»“æ„
                search_result = {
                    "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
                    "keyword": keyword,
                    "total_results": len(response['data']),
                    "results": response['data']
                }
                
                all_results.append(search_result)
            else:
                print("\nâš ï¸  æœç´¢å¤±è´¥æˆ–æœªæå–åˆ°æœ‰æ•ˆç»“æœ")
                return (False, [])
        
        return (True, all_results)

    except Exception as e:
        print(f"\nå‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}")
        return (False, [])

if __name__ == '__main__':
    main()